{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T08:08:22.527754Z",
     "start_time": "2020-09-01T08:08:22.519997Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import sys, os\n",
    "# システムの探索パスに'../../codes/'を追加し、codes/data.pyをモジュールとして読み込めるようにする\n",
    "sys.path.append('../../codes/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rcParams\n",
    "rcParams['font.family'] = 'sans-serif'\n",
    "rcParams['font.sans-serif'] = ['Hiragino Maru Gothic Pro', 'Yu Gothic', 'Meirio', 'Takao', 'IPAexGothic', 'IPAPGothic', 'Noto Sans CJK JP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import gym\n",
    "import numpy as np\n",
    "import pdb\n",
    "import QLearning as ql\n",
    "\n",
    "#------------------\n",
    "# 1. 強化学習の環境の作成と変数の設定\n",
    "# 割引報酬和の系列格納\n",
    "sumReward = 0\n",
    "sumRewards = []\n",
    "\n",
    "# ε-greedyの初期化\n",
    "epsilon = 0.5\n",
    "\n",
    "# 状態の分割数\n",
    "nSplit=50\n",
    "\n",
    "# 反復回数\n",
    "nIte = 50001\n",
    "# nIte = 10001\n",
    "#------------------\n",
    "\n",
    "#------------------\n",
    "# 2. QLearningクラスのインスタンス化\n",
    "agent = ql.QLearning(env='MountainCar-v0',gamma=0.99,nSplit=nSplit)\n",
    "#------------------\n",
    "\n",
    "#------------------\n",
    "# 3. Q学習のエピソードのループ\n",
    "frames = []\n",
    "\n",
    "for episode in np.arange(nIte):\n",
    "    \n",
    "    # 4. 環境の初期化\n",
    "    x = agent.reset()\n",
    "\n",
    "    # 5. ε-方策のεの値を減衰\n",
    "    epsilon *= 0.999\n",
    "        \n",
    "    # 6. Q学習のステップのループ\n",
    "    while(1):\n",
    "        \n",
    "        # 7. 行動を選択\n",
    "        y = agent.selectAction(x,epsilon=epsilon)\n",
    "\n",
    "        # 8. 行動を実行\n",
    "        next_x,r,done = agent.doAction(y)\n",
    "\n",
    "        # 9. Qテーブルの更新\n",
    "        agent.update(x,y,next_x,r)\n",
    "\n",
    "        # 10. 環境の描画\n",
    "        # if not episode == 5000:\n",
    "            # agent.draw()\n",
    "        # 10. 環境の描画\n",
    "        if  episode % 10000 == 0:\n",
    "                # Render into buffer. \n",
    "            frames.append(agent.env.render(mode = 'rgb_array'))\n",
    "            action = agent.env.action_space.sample()\n",
    "            observation, reward, done, info = agent.env.step(action)\n",
    "            \n",
    "        # 11. 状態の更新\n",
    "        x = next_x\n",
    "\n",
    "        # 12. 報酬和の計算、初期化および記録\n",
    "        if not done:\n",
    "            sumReward += agent.gamma**agent.step * r\n",
    "        else:\n",
    "            sumRewards.append(sumReward)\n",
    "            sumReward = 0\n",
    "            break\n",
    "            \n",
    "    if episode % 5000 == 0:\n",
    "        print(f\"Episode:{episode},sum of rewards:{sumRewards[-1]}\")\n",
    "        agent.plotEval(sumRewards)\n",
    "        agent.plotModel2D(xLabel=\"位置\",yLabel=\"速度\", fName=\"\")\n",
    "\n",
    "    # 13. 強化学習環境の終了\n",
    "    agent.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import HTML\n",
    "\n",
    "patch = plt.imshow(frames[0])\n",
    "\n",
    "def animate(i):\n",
    "    patch.set_data(frames[i])\n",
    "\n",
    "anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim.save(\"QLearning.gif\", writer = 'imagemagick')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "成功しなかったので追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames2 = []\n",
    "\n",
    "for episode in np.arange(10001):\n",
    "    \n",
    "    # 4. 環境の初期化\n",
    "    x = agent.reset()\n",
    "\n",
    "    # 5. ε-方策のεの値を減衰\n",
    "    epsilon *= 0.999\n",
    "        \n",
    "    # 6. Q学習のステップのループ\n",
    "    while(1):\n",
    "        \n",
    "        # 7. 行動を選択\n",
    "        y = agent.selectAction(x,epsilon=epsilon)\n",
    "\n",
    "        # 8. 行動を実行\n",
    "        next_x,r,done = agent.doAction(y)\n",
    "\n",
    "        # 9. Qテーブルの更新\n",
    "        agent.update(x,y,next_x,r)\n",
    "\n",
    "        # 10. 環境の描画\n",
    "        # if not episode == 5000:\n",
    "            # agent.draw()\n",
    "        # 10. 環境の描画\n",
    "        if  episode % 1000 == 0:\n",
    "                # Render into buffer. \n",
    "            frames2.append(agent.env.render(mode = 'rgb_array'))\n",
    "            action = agent.env.action_space.sample()\n",
    "            observation, reward, done, info = agent.env.step(action)\n",
    "            \n",
    "        # 11. 状態の更新\n",
    "        x = next_x\n",
    "\n",
    "        # 12. 報酬和の計算、初期化および記録\n",
    "        if not done:\n",
    "            sumReward += agent.gamma**agent.step * r\n",
    "        else:\n",
    "            sumRewards.append(sumReward)\n",
    "            sumReward = 0\n",
    "            break\n",
    "            \n",
    "    if episode % 5000 == 0:\n",
    "        print(f\"Episode:{episode},sum of rewards:{sumRewards[-1]}\")\n",
    "        agent.plotEval(sumRewards)\n",
    "        agent.plotModel2D(xLabel=\"位置\",yLabel=\"速度\", fName=\"\")\n",
    "\n",
    "    # 13. 強化学習環境の終了\n",
    "    agent.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "後半うまく行った"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import HTML\n",
    "\n",
    "patch = plt.imshow(frames2[0])\n",
    "\n",
    "def animate(i):\n",
    "    patch.set_data(frames2[i])\n",
    "\n",
    "anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames2), interval=50)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim.save(\"QLearning_success.gif\", writer = 'imagemagick')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
